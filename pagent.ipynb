{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um Agente de Pandas\n",
    "\n",
    "O Langchain não tem suporte, atualmente, pra criar novas variáveis com o agente de Pandas.\n",
    "\n",
    "Acho que dá pra criar um agente mais simples que escreva código e dê pra gente as variáveis que ele criou.\n",
    "\n",
    "Modelos OpenAI:\n",
    "\n",
    "1. \"gpt-4\": 8192,\n",
    "1. \"gpt-4-0314\": 8192,\n",
    "1. \"gpt-4-32k\": 32768,\n",
    "1. \"gpt-4-32k-0314\": 32768,\n",
    "1. \"gpt-3.5-turbo\": 4096,\n",
    "1. \"gpt-3.5-turbo-0301\": 4096,\n",
    "1. \"text-ada-001\": 2049,\n",
    "1. \"ada\": 2049,\n",
    "1. \"text-babbage-001\": 2040,\n",
    "1. \"babbage\": 2049,\n",
    "1. \"text-curie-001\": 2049,\n",
    "1. \"curie\": 2049,\n",
    "1. \"davinci\": 2049,\n",
    "1. \"text-davinci-003\": 4097,\n",
    "1. \"text-davinci-002\": 4097,\n",
    "1. \"code-davinci-002\": 8001,\n",
    "1. \"code-davinci-001\": 8001,\n",
    "1. \"code-cushman-002\": 2048,\n",
    "1. \"code-cushman-001\": 2048,\n",
    "\n",
    "Idéia:\n",
    "1. Agente Root: Recebe tarefa do usuário. Cria plano. Envia plano pra segundo agente.\n",
    "    1. Ao receber a resposta do leaf, decide se está bom.\n",
    "        1. Está bom: responde pro usuário e termina.\n",
    "        1. Não está bom: devolve problemas pro leaf\n",
    "1. Agente Leaf: Recebe plano do root.\n",
    "    1. Posso escrever código que resolve esse plano?\n",
    "        1. Sim -> Escreve o código e retorna pro root\n",
    "        1. Não -> Cria outros agentes leaf, e se torna um agente stem\n",
    "\n",
    "Perguntas:\n",
    "1. Quais são os tipos de respostas dos leaves?\n",
    "1. Qual é o tipo de resposta que o usuário recebe?\n",
    "1. Quais são os passos de executar a tarefa? Tipo Plano-Execução-Debug-Resposta. Algo assim?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversa com GPT-4\n",
    "\n",
    "Ivan:\n",
    "\n",
    "Hello chat. I want to create python code that creates \"agents\" which will receive a pandas related task and will solve it. This would use LLMs and code execution from inside python.\n",
    "\n",
    "My current idea is this: Have LLMs in a tree graph structure, and each node can either write code to solve a task it was asked to, or generate new nodes to solve steps which can solve the task it was asked to solve.\n",
    "\n",
    "The first node is the root. It receives a user prompt such as \"Load df.parquet and give me only the columns for which id==4\". Somehow I have to prompt an LLM to achive this:\n",
    "\n",
    "- Either it creates code to solve the task, and prompts another LLM instance (another node) to check whether the task is done or if it contains a mistake\n",
    "- Or it realizes the task is too complex to solve in just one prompt, and creates a list of steps. It then creates as many LLM instances as it needs to do each step and return to it the result. It then checks if the result return answers the user prompt. If so, it returns the answer to the user. Otherwise, it keeps correcting mistakes or updating its plans.\n",
    "\n",
    "Right now I'm just thinking of how to go about creating this agent graph structure. It's still not crystal clear to me how exactly I should prompt agents and what to ask of them in each step of this process. What do you think?\n",
    "\n",
    "---\n",
    "GPT-4:\n",
    "\n",
    "Your concept sounds fascinating! You're proposing a system that uses large language models (LLMs) to automate code generation and error checking for pandas-related tasks. Here's a sketch of a structure that could be used to implement this:\n",
    "\n",
    "**Data structures:**\n",
    "\n",
    "1. **Agent**: Represents a task-solving entity. It can be represented by an instance of an LLM or a function that generates LLM prompts. Each agent should have a method to:\n",
    "    - Receive tasks\n",
    "    - Generate a solution or breakdown the problem\n",
    "    - Verify if a task is done or contains mistakes\n",
    "    - Return the solution or the results of its tasks\n",
    "\n",
    "2. **Task**: Represents a pandas-related task. This could be a class with properties such as:\n",
    "    - The task prompt\n",
    "    - The current state of the solution\n",
    "    - The complexity level of the task\n",
    "\n",
    "3. **Tree**: A structure to represent your hierarchy of agents. Each node in the tree will correspond to an agent, and the edges will represent the relationship between agents, i.e., which agent generated which other agents.\n",
    "\n",
    "**Logic flow:**\n",
    "\n",
    "1. You give the root node agent the task.\n",
    "\n",
    "2. The root node agent, based on the task's complexity, either:\n",
    "    - Generates the solution directly, or\n",
    "    - Breaks down the task into subtasks. It then creates child nodes (new agents) and assigns each one of them a subtask. \n",
    "\n",
    "3. Each child node agent then follows the same process: it either solves the subtask or breaks it down further. \n",
    "\n",
    "4. Once a node comes up with a solution, it passes it to another LLM (could be a child node or a peer node) for error checking. \n",
    "\n",
    "5. Once all subtasks are done, the root node collects all the sub-solutions, compiles them into a single solution, and passes it back to the user.\n",
    "\n",
    "6. If the solution does not fulfill the user's requirements, the root node updates its plans and repeats the process.\n",
    "\n",
    "**How to prompt LLMs:**\n",
    "\n",
    "You can use a question-and-answer format to prompt the LLMs. Here's a rough idea:\n",
    "\n",
    "- **Task-solving prompt**: \"Given the task '{task.prompt}', write Python code using pandas that can solve this task.\"\n",
    "- **Error-checking prompt**: \"Given this Python code '{code}', does it correctly solve the task '{task.prompt}'? If not, what are the errors and how can they be corrected?\"\n",
    "\n",
    "You would need to wrap these prompts with code that feeds them into the LLM and handles the output.\n",
    "\n",
    "Remember that this is a conceptual sketch and a complex project. Depending on the specifics of your task, your environment, and your resources, you might need to adjust it. Also note that this approach can be CPU and memory intensive, especially if the tasks are complex or the tree becomes large. The LLMs used (like GPT-3 or GPT-4) can also generate incorrect or unoptimal solutions, and might require monitoring and fine-tuning. \n",
    "\n",
    "Furthermore, it's important to remember that the pandas library must be properly installed and working in the environment in which this code will be running. Code generation should also follow good security practices, as running dynamically generated code can present risks.\n",
    "\n",
    "Let me know if you'd like more help with any specific part of this project!\n",
    "\n",
    "---\n",
    "Ivan:\n",
    "\n",
    "Very nice thoughts! Thanks!\n",
    "\n",
    "So the task object can contain tasks inside itself as well. This can be a two-way link between the parent and child tasks. It can also contain the python/pandas objects related to it, as well as code that was used to generate each object. Actually, one \"code\" object can generate many python/pandas objects, so I can also create a \"code\" object, which a task can contain.\n",
    "\n",
    "Each agent can contain a history of prompts that were given to it, as well as its answers. It can also contain tasks and code that it generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "# openai.organization = \"org-IpivVcfezFhHNrRgT5zhR0Ng\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hello!\n",
      "\n",
      "Hello!\n",
      "\n",
      "Hello!\n",
      "\n",
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    model='text-ada-001',\n",
    "    prompt=\"Hello!\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"system\": \"You are a Senior Data Analyst and expert in Pandas. You are always concise and precise.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedir Plano de Ação"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def create_plan_prompt(task):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plan_prompt(task):\n",
    "    return f\"Create a numbered list with the precise steps to fulfil the task below. Do not write anything else.\\nTask: {task}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Import pandas library\n",
      "2. Create a dictionary with two lists as values, each list containing two random values\n",
      "3. Convert the dictionary to a dataframe using the pandas DataFrame() function\n",
      "4. Print the resulting dataframe to verify the two random columns and two random rows\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a Senior Data Analyst and expert in Pandas.\"},\n",
    "        {\"role\": \"user\", \"content\": create_plan_prompt(\"Create a dataframe with two random columns and two random rows\")},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Import pandas library',\n",
       " '2. Create a dictionary with two lists as values, each list containing two random values',\n",
       " '3. Convert the dictionary to a dataframe using the pandas DataFrame() function',\n",
       " '4. Print the resulting dataframe to verify the two random columns and two random rows']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = response.choices[0].message.content\n",
    "tasks.split('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def prompt_openai(task,prompt_design_fn = None):\n",
    "## def create_action_prompt(task):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_openai(task,prompt_design_fn = None):\n",
    "    task_msg = prompt_design_fn(task) if prompt_design_fn else task\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a Senior Data Analyst and expert in Pandas.\"},\n",
    "            {\"role\": \"user\", \"content\": task_msg},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "def create_action_prompt(task):\n",
    "    return f\"Write Python code to fulfil the task below. Do not write anything else.\\nTask: {task}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Import pandas library'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = tasks.split('\\n')\n",
    "tasks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Import pandas library',\n",
       " '2. Create a dictionary with two lists as values, each list containing two random values',\n",
       " '3. Convert the dictionary to a dataframe using the pandas DataFrame() function',\n",
       " '4. Print the resulting dataframe to verify the two random columns and two random rows']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = []\n",
    "\n",
    "code.append(prompt_openai(tasks[0], create_action_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7Lkj7Af4c8KUSeKLPONPRdPSiU19j at 0x7f3ef97d5da0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"```python\\nimport pandas as pd\\n```\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1685418861,\n",
       "  \"id\": \"chatcmpl-7Lkj7Af4c8KUSeKLPONPRdPSiU19j\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 9,\n",
       "    \"prompt_tokens\": 48,\n",
       "    \"total_tokens\": 57\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "code.append(prompt_openai(tasks[1], create_action_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7LkjFSwjQgVik6njynmRCkDlwsICj at 0x7f3ef97d5df0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"```python\\nimport random\\ndict_ = {}\\n\\nlist1 = [random.randint(1, 10) for i in range(2)]\\nlist2 = [random.randint(1, 10) for i in range(2)]\\n\\ndict_['list1'] = list1\\ndict_['list2'] = list2\\n\\nprint(dict_)\\n```\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1685418869,\n",
       "  \"id\": \"chatcmpl-7LkjFSwjQgVik6njynmRCkDlwsICj\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 72,\n",
       "    \"prompt_tokens\": 60,\n",
       "    \"total_tokens\": 132\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "code.append(prompt_openai(tasks[2], create_action_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7Lkjp8AjfYRO4bhmNFzmYeve0gmSc at 0x7f3ef974b380> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"```python\\nimport pandas as pd\\n\\n# assuming the dictionary is stored in the variable 'my_dict'\\ndf = pd.DataFrame(my_dict)\\n```\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1685418905,\n",
       "  \"id\": \"chatcmpl-7Lkjp8AjfYRO4bhmNFzmYeve0gmSc\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 29,\n",
       "    \"prompt_tokens\": 57,\n",
       "    \"total_tokens\": 86\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "code.append(prompt_openai(tasks[3], create_action_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7LkkDbqy94WVsW0A1kOALoWmmk3Pu at 0x7f3ef97676f0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"I am assuming that you have already created a Pandas DataFrame and want to print two random columns and two random rows of that DataFrame. Here is the code to achieve that:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\n# create a sample dataframe\\ndf = pd.DataFrame(np.random.randint(0,100,size=(10, 5)), columns=list('ABCDE'))\\n\\n# select two random columns and two random rows\\ncols = np.random.choice(df.columns, 2, replace=False)\\nrows = np.random.choice(df.index, 2, replace=False)\\n\\n# print the resulting dataframe\\nprint(df.loc[rows,cols])\\n```\\n\\nThis will print a DataFrame containing only the two random columns and two random rows you selected.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1685418929,\n",
       "  \"id\": \"chatcmpl-7LkkDbqy94WVsW0A1kOALoWmmk3Pu\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 149,\n",
       "    \"prompt_tokens\": 59,\n",
       "    \"total_tokens\": 208\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code = []\n",
    "# for task in tasks:\n",
    "#     code.append(prompt_openai(task, create_action_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "i: 0\n",
      "import pandas as pd\n",
      "\n",
      "--------------------------------------------------\n",
      "i: 1\n",
      "import random\n",
      "dict_ = {}\n",
      "\n",
      "list1 = [random.randint(1, 10) for i in range(2)]\n",
      "list2 = [random.randint(1, 10) for i in range(2)]\n",
      "\n",
      "dict_['list1'] = list1\n",
      "dict_['list2'] = list2\n",
      "\n",
      "print(dict_)\n",
      "\n",
      "--------------------------------------------------\n",
      "i: 2\n",
      "import pandas as pd\n",
      "\n",
      "# assuming the dictionary is stored in the variable 'my_dict'\n",
      "df = pd.DataFrame(my_dict)\n",
      "\n",
      "--------------------------------------------------\n",
      "i: 3\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# create a sample dataframe\n",
      "df = pd.DataFrame(np.random.randint(0,100,size=(10, 5)), columns=list('ABCDE'))\n",
      "\n",
      "# select two random columns and two random rows\n",
      "cols = np.random.choice(df.columns, 2, replace=False)\n",
      "rows = np.random.choice(df.index, 2, replace=False)\n",
      "\n",
      "# print the resulting dataframe\n",
      "print(df.loc[rows,cols])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, act in enumerate(code):\n",
    "    print('-'*50)\n",
    "    print(f'i: {i}')\n",
    "    print(act.choices[0].message.content.split('```')[-2].replace('python\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import random\\ndict_ = {}\\n\\nlist1 = [random.randint(1, 10) for i in range(2)]\\nlist2 = [random.randint(1, 10) for i in range(2)]\\n\\ndict_['list1'] = list1\\ndict_['list2'] = list2\\n\\nprint(dict_)\\n\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = code[1]\n",
    "act = act.choices[0].message.content.split('```')[-2].replace('python\\n', '')\n",
    "act"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def exec_code(code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "code = '''\n",
    "x = 10\n",
    "y = 20\n",
    "a = 2.7\n",
    "z = x + y + a\n",
    "'''\n",
    "exec(code)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 10, 'y': 20, 'a': 2.7, 'z': 32.7}\n",
      "a = 5.5\n"
     ]
    }
   ],
   "source": [
    "# Exec no globals\n",
    "def exec_code(code):\n",
    "    variables = {}\n",
    "    exec(code, {}, variables)\n",
    "    return variables\n",
    "\n",
    "code = '''\n",
    "x = 10\n",
    "y = 20\n",
    "a = 2.7\n",
    "z = x + y + a\n",
    "'''\n",
    "a = 5.5\n",
    "result = exec_code(code)\n",
    "print(result)\n",
    "print(f'a = {a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = \"import random\\ndict_ = {}\\n\\nlist1 = [random.randint(1, 10) for i in range(2)]\\nlist2 = [random.randint(1, 10) for i in range(2)]\\n\\ndict_['list1'] = list1\\ndict_['list2'] = list2\\n\\nprint(dict_)\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import random\n",
      "dict_ = {}\n",
      "\n",
      "list1 = [random.randint(1, 10) for i in range(2)]\n",
      "list2 = [random.randint(1, 10) for i in range(2)]\n",
      "\n",
      "dict_['list1'] = list1\n",
      "dict_['list2'] = list2\n",
      "\n",
      "print(dict_)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list1': [7, 4], 'list2': [8, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'random': <module 'random' from '/usr/lib/python3.10/random.py'>,\n",
       " 'dict_': {'list1': [7, 4], 'list2': [8, 1]},\n",
       " 'list1': [7, 4],\n",
       " 'list2': [8, 1]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exec_code(code):\n",
    "    local_namespace = {}\n",
    "    exec(code, globals(), local_namespace)\n",
    "    return local_namespace\n",
    "\n",
    "exec_code(act)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available RAM: 8.16 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def get_available_ram():\n",
    "    memory = psutil.virtual_memory()\n",
    "    available_ram = memory.available / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "    return available_ram\n",
    "\n",
    "# Example usage\n",
    "available_ram = get_available_ram()\n",
    "print(f\"Available RAM: {available_ram:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /media/ivanmasf/ext-main/llama-python/models/wizard-mega-13B.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 5594.59 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: [cublas] offloading 30 layers to GPU\n",
      "llama_model_load_internal: [cublas] total VRAM used: 9643 MB\n",
      "...........................................................................\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=\"/media/ivanmasf/ext-main/llama-python/models/wizard-mega-13B.ggmlv3.q8_0.bin\", verbose=True,n_gpu_layers=30,n_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  9515.84 ms\n",
      "llama_print_timings:      sample time =    28.76 ms /    63 runs   (    0.46 ms per token)\n",
      "llama_print_timings: prompt eval time =   602.43 ms /    10 tokens (   60.24 ms per token)\n",
      "llama_print_timings:        eval time = 10620.24 ms /    62 runs   (  171.29 ms per token)\n",
      "llama_print_timings:       total time = 11380.50 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-3f95c8db-2301-4ed1-9655-58b69585ab18',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1685836422,\n",
       " 'model': '/media/ivanmasf/ext-main/llama-python/models/wizard-mega-13B.ggmlv3.q8_0.bin',\n",
       " 'choices': [{'text': \"\\ndf = df.groupby('price')['date'].mean()\\n\\nGroup a numpy array on column 'price'\\narr = arr.reshape(-1,)\\narr = arr.groupby(np.arange(len(arr))//5).mean()\\n```</s>\",\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 11, 'completion_tokens': 62, 'total_tokens': 73}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm(\"Group a pandas dataframe on column \\'price\\' \", max_tokens=200, stop=[\"Q:\"])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df = df.groupby('price')['date'].mean()\n",
      "\n",
      "Group a numpy array on column 'price'\n",
      "arr = arr.reshape(-1,)\n",
      "arr = arr.groupby(np.arange(len(arr))//5).mean()\n",
      "```</s>\n"
     ]
    }
   ],
   "source": [
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
